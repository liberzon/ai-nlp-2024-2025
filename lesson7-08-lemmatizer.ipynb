{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 22.11.1\r\n",
      "  latest version: 24.1.1\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.1\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:37.525279Z",
     "start_time": "2024-02-13T17:49:24.804671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 22.11.1\r\n",
      "  latest version: 24.1.1\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.1\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y spacy-lookups-data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:43.741819Z",
     "start_time": "2024-02-13T17:49:37.527162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:47.002207Z",
     "start_time": "2024-02-13T17:49:43.742872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:47.007921Z",
     "start_time": "2024-02-13T17:49:47.005338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemmatizer          \n",
      "friend              friend              \n",
      "friendship          friendship          \n",
      "friends             friend              \n",
      "friendships         friendship          \n",
      "stabil              stabil              \n",
      "destabilize         destabilize         \n",
      "misunderstanding    misunderstanding    \n",
      "railroad            railroad            \n",
      "moonlight           moonlight           \n",
      "football            football            \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemmatizer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}\".format(word, lemmatizer.lemmatize(word)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:56.641074Z",
     "start_time": "2024-02-13T17:49:56.627234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks: rock\n",
      "corpora: corpus\n",
      "indices: index\n",
      "better: good\n",
      "better: better\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks:\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora:\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"indices:\", lemmatizer.lemmatize(\"indices\"))\n",
    "print(\"better:\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better:\", lemmatizer.lemmatize(\"better\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:47.805655Z",
     "start_time": "2024-02-13T17:49:47.802828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "bow_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:47.810228Z",
     "start_time": "2024-02-13T17:49:47.806746Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "sample_text = \"Apple is looking at buying U.K. startup for $1 billion\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:47.815280Z",
     "start_time": "2024-02-13T17:49:47.809263Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = nlp(sample_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:48.165548Z",
     "start_time": "2024-02-13T17:49:47.812167Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x168259f40>),\n ('tagger', <spacy.pipeline.tagger.Tagger at 0x168259fa0>),\n ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x168156c10>),\n ('attribute_ruler',\n  <spacy.pipeline.attributeruler.AttributeRuler at 0x1684065c0>),\n ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x16846f580>),\n ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x168156ba0>)]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:48.174712Z",
     "start_time": "2024-02-13T17:49:48.168816Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['Apple',\n 'is',\n 'looking',\n 'at',\n 'buying',\n 'U.K.',\n 'startup',\n 'for',\n '$',\n '1',\n 'billion']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = []\n",
    "for token in tokens:\n",
    "    token_list.append(token.text)\n",
    "token_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:48.175627Z",
     "start_time": "2024-02-13T17:49:48.172735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['Apple',\n 'be',\n 'look',\n 'at',\n 'buy',\n 'U.K.',\n 'startup',\n 'for',\n '$',\n '1',\n 'billion']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = []\n",
    "for token in tokens:\n",
    "    lemma_list.append(token.lemma_)\n",
    "lemma_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:48.180228Z",
     "start_time": "2024-02-13T17:49:48.176470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:49:48.180816Z",
     "start_time": "2024-02-13T17:49:48.179636Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
